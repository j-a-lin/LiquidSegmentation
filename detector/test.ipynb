{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of  testing examples: 200\n",
      "# of    object classes: 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from data import CTDataset\n",
    "from detector import PointNetDenseCls4D\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 1\n",
    "workers = 4\n",
    "threshold_min = 1700\n",
    "threshold_max = 2700\n",
    "npoints = 25000\n",
    "\n",
    "test_dataset = CTDataset(root='../data',\n",
    "                         threshold_min=int(threshold_min),\n",
    "                         threshold_max=int(threshold_max),\n",
    "                         npoints=npoints,\n",
    "                         train=False, dim4=True)\n",
    "\n",
    "print(\"# of  testing examples: {0}\".format(len(test_dataset)))\n",
    "num_classes = test_dataset.nclasses\n",
    "print(\"# of    object classes: {0}\".format(num_classes))\n",
    "\n",
    "testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"./pth/model_99.pth\"\n",
    "\n",
    "classifier = PointNetDenseCls4D(num_points=npoints, num_classes=num_classes)\n",
    "classifier.load_state_dict(torch.load(model))\n",
    "classifier.cuda()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, data = next(enumerate(testdataloader, 0))\n",
    "points, target, centroid, index = data\n",
    "points, target = Variable(points), Variable(target)\n",
    "points = points.transpose(2, 1)\n",
    "points, target = points.cuda(), target.cuda()\n",
    "pred, _ = classifier(points)\n",
    "pred = pred.view(-1, num_classes)\n",
    "target = target.view(-1)\n",
    "\n",
    "loss = F.nll_loss(pred, target)\n",
    "pred_choice = pred.data.max(1)[1]\n",
    "correct = pred_choice.eq(target.data).cpu().sum()\n",
    "print(\"test loss: {0} accuracy: {1}\".format(\n",
    "    loss.item(),\n",
    "    correct.item() / float(batch_size * npoints)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import visualize\n",
    "\n",
    "data = points[0, :, :].transpose(1, 0).cpu().numpy()\n",
    "prediction = pred_choice.view(batch_size, -1)[0, :].cpu().numpy()\n",
    "print(prediction.sum())\n",
    "visualize.scatter_with_target(data, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = target.view(batch_size, -1)[0, :].cpu().numpy()\n",
    "visualize.scatter_with_target(data, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confidence = pred.data.cpu().numpy()[0:npoints, :]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(confidence[:, 0], bins='auto')\n",
    "plt.figure()\n",
    "plt.hist(confidence[:, 1], bins='auto')\n",
    "\n",
    "intensities = confidence[:, 1]\n",
    "intensities = (intensities - intensities.min()) / (intensities.max() - intensities.min())\n",
    "visualize.scatter_with_intensities(data, intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import region_grow\n",
    "seed_idx = np.argmax(confidence[:, 1])\n",
    "print(seed_idx)\n",
    "seed = tuple(data[seed_idx][0:3].astype(np.int))\n",
    "print(seed)\n",
    "\n",
    "seed_mask = np.zeros(npoints)\n",
    "seed_mask[seed_idx] = 1\n",
    "visualize.scatter_with_target(data, seed_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, volume, _, _, _ = test_dataset.load(index[0])\n",
    "\n",
    "seg = region_grow(volume, seed, 1)\n",
    "\n",
    "result = np.argwhere(seg == True)\n",
    "\n",
    "visualize.scatter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/200]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-0f5939dd60de>\", line 7, in <module>\n",
      "    for i, data in enumerate(testdataloader, 0):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 280, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 259, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 732, in answer_challenge\n",
      "    message = connection.recv_bytes(256)         # reject large message\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 2743) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m"
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/visinf/home/alin/pointnetdetection3d/detector/data.py\", line 234, in __getitem__\n",
      "    sampled, target, volume, centroid, labels, num_labels = self.load(index)\n",
      "  File \"/visinf/home/alin/pointnetdetection3d/detector/data.py\", line 73, in load\n",
      "    image = itk.imread(data_file)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/itkExtras.py\", line 446, in imread\n",
      "    reader.Update()\n",
      "KeyboardInterrupt\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/visinf/home/alin/pointnetdetection3d/detector/data.py\", line 234, in __getitem__\n",
      "    sampled, target, volume, centroid, labels, num_labels = self.load(index)\n",
      "  File \"/visinf/home/alin/pointnetdetection3d/detector/data.py\", line 73, in load\n",
      "    image = itk.imread(data_file)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/itkExtras.py\", line 446, in imread\n",
      "    reader.Update()\n",
      "KeyboardInterrupt\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/visinf/home/alin/pointnetdetection3d/detector/data.py\", line 234, in __getitem__\n",
      "    sampled, target, volume, centroid, labels, num_labels = self.load(index)\n",
      "  File \"/visinf/home/alin/pointnetdetection3d/detector/data.py\", line 73, in load\n",
      "    image = itk.imread(data_file)\n",
      "  File \"/visinf/home/alin/miniconda3/envs/thesis/lib/python3.6/site-packages/itkExtras.py\", line 446, in imread\n",
      "    reader.Update()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# quantitative evaluation\n",
    "import numpy as np\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "predictions = np.zeros((0, npoints))\n",
    "targets = np.zeros((0, npoints))\n",
    "for i, data in enumerate(testdataloader, 0):\n",
    "    clear_output()\n",
    "    print(\"[{0}/{1}]\".format(i+1, len(testdataloader)))\n",
    "    points, target, centroid, index = data\n",
    "    points, target = Variable(points), Variable(target)\n",
    "    points = points.transpose(2, 1)\n",
    "    points, target = points.cuda(), target.cuda()\n",
    "    #pred, _ = classifier(points)\n",
    "    #pred = pred.view(-1, num_classes)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    #loss = F.nll_loss(pred, target)\n",
    "    #pred_choice = pred.data.max(1)[1]\n",
    "    \n",
    "    #prediction = pred_choice.view(batch_size, -1).data.cpu().numpy()\n",
    "    prediction = np.random.randint(2, size=(batch_size, npoints))\n",
    "    target = target.view(batch_size, -1).data.cpu().numpy()\n",
    "    \n",
    "    predictions = np.append(predictions, prediction, axis=0)\n",
    "    targets = np.append(targets, target, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158/200]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bad3bb8023aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_grow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pointnetdetection3d/detector/utils.py\u001b[0m in \u001b[0;36mregion_grow\u001b[0;34m(img, seed, t, max_iter)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mjmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mkmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mkmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimax\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mjmax\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkmax\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import utils\n",
    "# quantitative evaluation\n",
    "import numpy as np\n",
    "from IPython.core.display import clear_output\n",
    "\n",
    "predictions = np.zeros((0, npoints))\n",
    "targets = np.zeros((0, npoints))\n",
    "for i in range(len(test_dataset)):\n",
    "    clear_output()\n",
    "    print(\"[{0}/{1}]\".format(i+1, len(test_dataset)))\n",
    "    sampled, target, volume, _, _, _ = test_dataset.load(i)\n",
    "    \n",
    "    intensities = np.array(list(map(lambda d: volume[d[0], d[1], d[2]], sampled)))\n",
    "    points = np.concatenate((sampled, np.reshape(intensities, (-1, 1))), axis=1)\n",
    "    \n",
    "    seed, f = utils.select_seed(points)\n",
    "    seg = utils.region_grow(volume, seed, 1)\n",
    "    prediction = seg[sampled[:, 0], sampled[:, 1], sampled[:, 2]].astype(np.int)\n",
    "    \n",
    "    predictions = np.append(predictions, np.reshape(prediction, (1, -1)), axis=0)\n",
    "    targets = np.append(targets, np.reshape(target, (1, -1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mean_ap25: 0.6496815286624203\n",
      "  mean_ap50: 0.2929936305732484\n",
      "  mean_ap75: 0.0\n",
      "average_iou: 0.33175912896258675\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluation\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "mean_ap25 = evaluation.mean_average_precision(predictions, targets, iou_threshold=0.25)\n",
    "mean_ap50 = evaluation.mean_average_precision(predictions, targets, iou_threshold=0.5)\n",
    "mean_ap75 = evaluation.mean_average_precision(predictions, targets, iou_threshold=0.75)\n",
    "average_iou = evaluation.average_iou(predictions, targets)\n",
    "\n",
    "print(\"  mean_ap25: {0}\".format(mean_ap25))\n",
    "print(\"  mean_ap50: {0}\".format(mean_ap50))\n",
    "print(\"  mean_ap75: {0}\".format(mean_ap75))\n",
    "print(\"average_iou: {0}\".format(average_iou))\n",
    "\n",
    "### Random @ 10k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.0684\n",
    "#   mean_ap50: 0.0000\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.1096\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.0900\n",
    "#   mean_ap50: 0.0000\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.1103\n",
    "\n",
    "### Random @ 25k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.0703\n",
    "#   mean_ap50: 0.0000\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.1097\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.0900\n",
    "#   mean_ap50: 0.0000\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.1104\n",
    "\n",
    "### Random @ 50k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.0703\n",
    "#   mean_ap50: 0.0000\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.1097\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.0900\n",
    "#   mean_ap50: 0.0000\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.1103\n",
    "\n",
    "\n",
    "### PointNet 4D Segmentation @ 10k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.2246\n",
    "#   mean_ap50: 0.0859\n",
    "#   mean_ap75: 0.0547\n",
    "# average_iou: 0.1598\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.1950\n",
    "#   mean_ap50: 0.1000\n",
    "#   mean_ap75: 0.0800\n",
    "# average_iou: 0.1624\n",
    "\n",
    "### PointNet 4D Segmentation @ 25k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.2324\n",
    "#   mean_ap50: 0.0781\n",
    "#   mean_ap75: 0.0508\n",
    "# average_iou: 0.1663\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.2500\n",
    "#   mean_ap50: 0.1000\n",
    "#   mean_ap75: 0.0750\n",
    "# average_iou: 0.1760\n",
    "\n",
    "### PointNet 4D Segmentation @ 50k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.2070\n",
    "#   mean_ap50: 0.0586\n",
    "#   mean_ap75: 0.0254\n",
    "# average_iou: 0.1381\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.2150\n",
    "#   mean_ap50: 0.0800\n",
    "#   mean_ap75: 0.0350\n",
    "# average_iou: 0.1481\n",
    "\n",
    "### Density-Based Seeded Region Growing @ 25k\n",
    "## Training Data:\n",
    "#   mean_ap25: 0.\n",
    "#   mean_ap50: 0.\n",
    "#   mean_ap75: 0.\n",
    "# average_iou: 0.\n",
    "## Testing Data:\n",
    "#   mean_ap25: 0.6497\n",
    "#   mean_ap50: 0.2930\n",
    "#   mean_ap75: 0.0000\n",
    "# average_iou: 0.3318"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}